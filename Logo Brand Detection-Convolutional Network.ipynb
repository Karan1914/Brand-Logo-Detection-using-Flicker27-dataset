{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using Flick-27 logo dataset which is publicly available and has images download from Flickr of 27 brands. It has 30 images for each class. In total 810 images for training. Although it is a very small dataset for Deep Learning problem so I will use Data Augmentation techniques that can inflated the data to a bigger dataset suitable for training an object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 32\n",
    "height = 32\n",
    "\n",
    "posshiftshift_min = -5\n",
    "posshiftshift_max = 5\n",
    "scales = [0.9, 1.1]\n",
    "rot_min = -15\n",
    "rot_max = 15\n",
    "\n",
    "dir = 'flickrData'\n",
    "imgdir = os.path.join(dir, 'flickr_logos_27_dataset_images')\n",
    "pp_dir = os.path.join(\n",
    "    dir, 'processedF')\n",
    "annot = 'flickr_logos_27_dataset_training_set_annotation.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_annotation: 4536, 7 \n"
     ]
    }
   ],
   "source": [
    "annot_train = np.loadtxt(os.path.join(dir, annot), dtype='a')\n",
    "print('train_annotation: %d, %d ' % (annot_train.shape))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop and augmente data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function for the augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annot(annot):\n",
    "    fn = annot[0].decode('utf-8')\n",
    "    class_name = annot[1].decode('utf-8')\n",
    "    train_subset_class = annot[2].decode('utf-8')\n",
    "    return fn, class_name, train_subset_class\n",
    "\n",
    "# extarcting the x and y coordinates of the logo given in the txt file\n",
    "def get_rect(annot):             \n",
    "    rect = defaultdict(int)\n",
    "    x1, y1, x2, y2 = rect_coord(annot[3:])\n",
    "    cx, cy, wid, hgt = center_wid_hgt(x1, y1, x2, y2)\n",
    "    rect['x1'] = x1\n",
    "    rect['y1'] = y1\n",
    "    rect['x2'] = x2\n",
    "    rect['y2'] = y2\n",
    "    rect['cx'] = cx\n",
    "    rect['cy'] = cy\n",
    "    rect['wid'] = wid\n",
    "    rect['hgt'] = hgt\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting position of the extracted logo.\n",
    "def posshift(annot, im):   \n",
    "    posshift_ims = []\n",
    "    posshift_suffixes = []\n",
    "\n",
    "    rect = get_rect(annot)\n",
    "    for sx, sy in product(                        \n",
    "            range(posshiftshift_min, posshiftshift_max),\n",
    "            range(posshiftshift_min, posshiftshift_max)):\n",
    "        cx = rect['cx'] + sx\n",
    "        cy = rect['cy'] + sy\n",
    "        cropped_im = im.crop((cx - rect['wid'] // 2, cy - rect['hgt'] // 2,\n",
    "                              cx + rect['wid'] // 2, cy + rect['hgt'] // 2))\n",
    "        resized_im = cropped_im.resize((width, height))\n",
    "        posshift_ims.append(resized_im)\n",
    "        posshift_suffixes.append('p' + str(sx) + str(sy))\n",
    "        cropped_im.close()\n",
    "\n",
    "    return posshift_ims, posshift_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For resizing the extracted logo.\n",
    "def scale(annot, im):   \n",
    "    scale_ims = []\n",
    "    scale_suffixes = []\n",
    "\n",
    "    rect = get_rect(annot)\n",
    "    for s in scales:\n",
    "        w = int(rect['wid'] * s)\n",
    "        h = int(rect['hgt'] * s)\n",
    "        cropped_im = im.crop((rect['cx'] - w // 2, rect['cy'] - h // 2,\n",
    "                              rect['cx'] + w // 2, rect['cy'] + h // 2))\n",
    "        resized_im = cropped_im.resize((width, height))\n",
    "        scale_ims.append(resized_im)\n",
    "        scale_suffixes.append('s' + str(s))\n",
    "        cropped_im.close()\n",
    "\n",
    "    return scale_ims, scale_suffixes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotating the extracted logo.\n",
    "def rotate(annot, im):   \n",
    "    rotate_ims = []\n",
    "    rotate_suffixes = []\n",
    "\n",
    "    rect = get_rect(annot)\n",
    "    for r in range(rot_min, rot_max):\n",
    "        rotated_im = im.rotate(r)\n",
    "        cropped_im = rotated_im.crop(\n",
    "            (rect['cx'] - rect['wid'] // 2, rect['cy'] - rect['hgt'] // 2,\n",
    "             rect['cx'] + rect['wid'] // 2, rect['cy'] + rect['hgt'] // 2))\n",
    "        resized_im = cropped_im.resize((width, height))\n",
    "        rotate_ims.append(resized_im)\n",
    "        rotate_suffixes.append('r' + str(r))\n",
    "        rotated_im.close()\n",
    "        cropped_im.close()\n",
    "\n",
    "    return rotate_ims, rotate_suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function for cropping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropping the logo\n",
    "def crop(annot, im):                        \n",
    "    x1, y1, x2, y2 = rect_coord(annot[3:])\n",
    "    cropped_im = im.crop((x1, y1, x2, y2))\n",
    "    cropped_im = cropped_im.resize((width, height))\n",
    "    cropped_suffix = 'p00'\n",
    "    return [cropped_im], [cropped_suffix]\n",
    "\n",
    "# apply on each list item and collect all the return values.\n",
    "def rect_coord(annot_part):\n",
    "    return list(map(int, annot_part))   \n",
    "\n",
    "\n",
    "def center_wid_hgt(x1, y1, x2, y2):\n",
    "    cx = x1 + (x2 - x1) // 2\n",
    "    cy = y1 + (y2 - y1) // 2\n",
    "    wid = (x2 - x1)\n",
    "    hgt = (y2 - y1)\n",
    "    return cx, cy, wid, hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_skip(annot_part):\n",
    "    x1, y1, x2, y2 = rect_coord(annot_part)\n",
    "    _, _, wid, hgt = center_wid_hgt(x1, y1, x2, y2)\n",
    "    if wid <= 0 or hgt <= 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Saving the processed image.\n",
    "def save_im(annot, cnt, *args):         \n",
    "    fn, class_name, train_subset_class = parse_annot(annot)\n",
    "    dst_dir = os.path.join(pp_dir, class_name)\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    for i, arg in enumerate(args):\n",
    "        for im, suffix in zip(arg[0], arg[1]):\n",
    "            save_fn = '_'.join([\n",
    "                fn.split('.')[0], class_name, train_subset_class, str(cnt),\n",
    "                suffix\n",
    "            ]) + os.path.splitext(fn)[1]\n",
    "            im.save(os.path.join(dst_dir, save_fn))\n",
    "\n",
    "\n",
    "def close_im(*args):\n",
    "    for ims in args:\n",
    "        for im in ims:\n",
    "            im.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # parent function which call all the sub functions.\n",
    "def crop_and_aug(annot_train):    \n",
    "    cnt_per_file = defaultdict(int)\n",
    "    for annot in annot_train:\n",
    "        # for generating a file name\n",
    "        fn, _, _ = parse_annot(annot)\n",
    "        cnt_per_file[fn] += 1\n",
    "\n",
    "        # skip if width or height equal zero\n",
    "        if is_skip(annot[3:]):\n",
    "            print('Skip: ', fn)\n",
    "            continue\n",
    "\n",
    "        # open an image\n",
    "        im = Image.open(os.path.join(imgdir, fn))\n",
    "\n",
    "        # normal cropping\n",
    "        cropped_ims, cropped_suffixes = crop(annot, im)\n",
    "\n",
    "        # augment by shifting a center\n",
    "        shifted_ims, shifted_suffixes = posshift(annot, im)\n",
    "\n",
    "        # augment by scaling\n",
    "        scaled_ims, scaled_suffixes = scale(annot, im)\n",
    "\n",
    "        # augment by rotation\n",
    "        rotated_ims, rotated_suffixes = rotate(annot, im)\n",
    "\n",
    "        # save images\n",
    "        save_im(annot, cnt_per_file[fn], [cropped_ims, cropped_suffixes],\n",
    "                [shifted_ims, shifted_suffixes], [scaled_ims, scaled_suffixes],\n",
    "                [rotated_ims, rotated_suffixes])\n",
    "\n",
    "        # close image file\n",
    "        close_im([im], cropped_ims, shifted_ims, scaled_ims, rotated_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_aug_with_none(annot_train, with_none=False):\n",
    "    # root directory to save processed images\n",
    "    if not os.path.exists(pp_dir):\n",
    "        os.makedirs(pp_dir)\n",
    "\n",
    "    # crop images and apply augmentation\n",
    "    crop_and_aug(annot_train)\n",
    "\n",
    "    # print results\n",
    "    org_imgs = [img for img in os.listdir(imgdir)]\n",
    "    crop_and_aug_imgs = [\n",
    "        fname\n",
    "        for root, dirs, files in os.walk(pp_dir)\n",
    "        for fname in glob.glob(os.path.join(root, '*.jpg'))  # look for the file with .jpg extension.\n",
    "    ]\n",
    "    print('original: %d' % (len(org_imgs)))\n",
    "    print('cropped: %d' % (len(crop_and_aug_imgs)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function for  splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For spliting dataset into 2 directories Train and Test.\n",
    "def do_train_test_split():                                   \n",
    "    class_names = [cls for cls in os.listdir(pp_dir)]\n",
    "   # create directories under a particular class name.\n",
    "    for class_name in class_names:                           \n",
    "        if os.path.exists(                                  \n",
    "                os.path.join(pp_dir, class_name, 'train')):\n",
    "            continue\n",
    "        if os.path.exists(\n",
    "                os.path.join(pp_dir, class_name, 'test')):\n",
    "            continue\n",
    "\n",
    "        imgs = [\n",
    "            img\n",
    "            for img in os.listdir(\n",
    "                os.path.join(pp_dir, class_name))\n",
    "        ]\n",
    "        # train=0.75, test=0.25\n",
    "        train_imgs, test_imgs = train_test_split(imgs)\n",
    "        # move images to train or test directory\n",
    "        # create directories\n",
    "        os.makedirs(os.path.join(pp_dir, class_name, 'train'))         \n",
    "        os.makedirs(os.path.join(pp_dir, class_name, 'test'))\n",
    "        for img in train_imgs:\n",
    "            dst = os.path.join(pp_dir, class_name, 'train')\n",
    "            src = os.path.join(pp_dir, class_name, img)\n",
    "            # moving image into that directory\n",
    "            shutil.move(src, dst)                                      \n",
    "        for img in test_imgs:\n",
    "            dst = os.path.join(pp_dir, class_name, 'test')\n",
    "            src = os.path.join(pp_dir, class_name, img)\n",
    "            shutil.move(src, dst)                                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip:  2662264721.jpg\n",
      "Skip:  2662264721.jpg\n",
      "Skip:  2662264721.jpg\n",
      "Skip:  2662264721.jpg\n",
      "Skip:  2662264721.jpg\n",
      "original: 1079\n",
      "cropped: 598092\n"
     ]
    }
   ],
   "source": [
    "# cropping and data augmentation\n",
    "crop_and_aug_with_none(annot_train)\n",
    "# train_test_split\n",
    "do_train_test_split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of the image\n",
    "width = 32     \n",
    "height = 32    \n",
    "channel = 3         \n",
    "pix_val = 255.0\n",
    "\n",
    "dir = 'flickrData'\n",
    "# Directory where processed images are stored\n",
    "pp_dir = os.path.join(dir, 'processedF') \n",
    "# Name of the pickle file\n",
    "pickle_file = 'logo_dataset.pickle'    \n",
    "# Number of images stored as training dataset in pickle file from the processed images\n",
    "train_size = 70000  \n",
    "val_size = 5000\n",
    "# Number of images stored as test dataset in pickle file from the processed images\n",
    "test_size = 7000    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates array of dataset\n",
    "\n",
    "def array(nb_rows, image_width, image_height, image_ch=1):     \n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray(                               #  stores its height, width and channel into an array\n",
    "            (nb_rows, image_height, image_width, image_ch), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)        #  stores its labels\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging pickle files of all the classes into one pickle file.\n",
    "def combine(pickle_files, train_size, val_size=0):     \n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = array(val_size, width,\n",
    "                                              height, channel)\n",
    "    train_dataset, train_labels = array(train_size, width,\n",
    "                                              height, channel)\n",
    "    vsize_per_class = val_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                logo_set = pickle.load(f)\n",
    "                np.random.shuffle(logo_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_logo = logo_set[:vsize_per_class, :, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :, :] = valid_logo\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                train_logo = logo_set[vsize_per_class:end_l, :, :, :]\n",
    "                train_dataset[start_t:end_t, :, :, :] = train_logo\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makepickle(train_dataset, train_labels, valid_dataset, valid_labels,   \n",
    "                test_dataset, test_labels):\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        save = {\n",
    "            'train_dataset': train_dataset,\n",
    "            'train_labels': train_labels,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'valid_labels': valid_labels,\n",
    "            'test_dataset': test_dataset,\n",
    "            'test_labels': test_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)      # Saving data of the images into a pickle file\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the image\n",
    "def load_logo(data_dir):    \n",
    "    image_files = os.listdir(data_dir)      \n",
    "    dataset = np.ndarray(\n",
    "        shape=(len(image_files), height, width, channel),\n",
    "        dtype=np.float32)\n",
    "    print(data_dir)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(data_dir, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) -        \n",
    "                          pix_val / 2) / pix_val\n",
    "            if image_data.shape != (height, width, channel):\n",
    "                raise Exception('Unexpected image shape: %s' %\n",
    "                                str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e,\n",
    "                  '-it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]                           \n",
    "    print('Full dataset tensor:', dataset.shape)       # Tell processed number of images for a particular class \n",
    "    print('Mean:', np.mean(dataset))                   # Calculate mean over that entire class\n",
    "    print('Standard deviation:', np.std(dataset))      # Calculate standard deviation over that entire class\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pickle file\n",
    "def pickling(data_dirs, force=False):     \n",
    "    dataset_names = []\n",
    "    for dir in data_dirs:\n",
    "        set_filename = dir + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "\n",
    "         \n",
    "        if os.path.exists(set_filename) and force:    \n",
    "    \n",
    "            print('%s already present - Skipping pickling. ' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_logo(dir)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME = [\n",
    "    'Apple', 'BMW','Heineken','HP','Intel','Mini','Starbucks','Vodafone', 'Citroen', 'Ferrari'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling flickrData\\processedF\\Apple\\train.pickle.\n",
      "flickrData\\processedF\\Apple\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\a\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (25443, 32, 32, 3)\n",
      "Mean: 0.116072275\n",
      "Standard deviation: 0.2986908\n",
      "Pickling flickrData\\processedF\\BMW\\train.pickle.\n",
      "flickrData\\processedF\\BMW\\train\n",
      "Full dataset tensor: (11781, 32, 32, 3)\n",
      "Mean: -0.07783215\n",
      "Standard deviation: 0.32733378\n",
      "Pickling flickrData\\processedF\\Heineken\\train.pickle.\n",
      "flickrData\\processedF\\Heineken\\train\n",
      "Full dataset tensor: (15939, 32, 32, 3)\n",
      "Mean: -0.08269091\n",
      "Standard deviation: 0.30394986\n",
      "Pickling flickrData\\processedF\\HP\\train.pickle.\n",
      "flickrData\\processedF\\HP\\train\n",
      "Full dataset tensor: (10989, 32, 32, 3)\n",
      "Mean: -0.11493689\n",
      "Standard deviation: 0.348335\n",
      "Pickling flickrData\\processedF\\Intel\\train.pickle.\n",
      "flickrData\\processedF\\Intel\\train\n",
      "Full dataset tensor: (12177, 32, 32, 3)\n",
      "Mean: 0.12077578\n",
      "Standard deviation: 0.3359874\n",
      "Pickling flickrData\\processedF\\Mini\\train.pickle.\n",
      "flickrData\\processedF\\Mini\\train\n",
      "Full dataset tensor: (10395, 32, 32, 3)\n",
      "Mean: -0.08043128\n",
      "Standard deviation: 0.31972796\n",
      "Pickling flickrData\\processedF\\Starbucks\\train.pickle.\n",
      "flickrData\\processedF\\Starbucks\\train\n",
      "Full dataset tensor: (18612, 32, 32, 3)\n",
      "Mean: -0.103952214\n",
      "Standard deviation: 0.29434156\n",
      "Pickling flickrData\\processedF\\Vodafone\\train.pickle.\n",
      "flickrData\\processedF\\Vodafone\\train\n",
      "Full dataset tensor: (26037, 32, 32, 3)\n",
      "Mean: 0.053974822\n",
      "Standard deviation: 0.33454582\n",
      "Pickling flickrData\\processedF\\Citroen\\train.pickle.\n",
      "flickrData\\processedF\\Citroen\\train\n",
      "Full dataset tensor: (20790, 32, 32, 3)\n",
      "Mean: 0.087700516\n",
      "Standard deviation: 0.3602663\n",
      "Pickling flickrData\\processedF\\Ferrari\\train.pickle.\n",
      "flickrData\\processedF\\Ferrari\\train\n",
      "Full dataset tensor: (10395, 32, 32, 3)\n",
      "Mean: -0.09901171\n",
      "Standard deviation: 0.33935654\n",
      "Pickling flickrData\\processedF\\Apple\\test.pickle.\n",
      "flickrData\\processedF\\Apple\\test\n",
      "Full dataset tensor: (8481, 32, 32, 3)\n",
      "Mean: 0.11713261\n",
      "Standard deviation: 0.2983104\n",
      "Pickling flickrData\\processedF\\BMW\\test.pickle.\n",
      "flickrData\\processedF\\BMW\\test\n",
      "Full dataset tensor: (3927, 32, 32, 3)\n",
      "Mean: -0.07773973\n",
      "Standard deviation: 0.32912585\n",
      "Pickling flickrData\\processedF\\Heineken\\test.pickle.\n",
      "flickrData\\processedF\\Heineken\\test\n",
      "Full dataset tensor: (5313, 32, 32, 3)\n",
      "Mean: -0.08586216\n",
      "Standard deviation: 0.30206856\n",
      "Pickling flickrData\\processedF\\HP\\test.pickle.\n",
      "flickrData\\processedF\\HP\\test\n",
      "Full dataset tensor: (3663, 32, 32, 3)\n",
      "Mean: -0.11147327\n",
      "Standard deviation: 0.3502512\n",
      "Pickling flickrData\\processedF\\Intel\\test.pickle.\n",
      "flickrData\\processedF\\Intel\\test\n",
      "Full dataset tensor: (4059, 32, 32, 3)\n",
      "Mean: 0.117239505\n",
      "Standard deviation: 0.33615553\n",
      "Pickling flickrData\\processedF\\Mini\\test.pickle.\n",
      "flickrData\\processedF\\Mini\\test\n",
      "Full dataset tensor: (3465, 32, 32, 3)\n",
      "Mean: -0.07715294\n",
      "Standard deviation: 0.32257822\n",
      "Pickling flickrData\\processedF\\Starbucks\\test.pickle.\n",
      "flickrData\\processedF\\Starbucks\\test\n",
      "Full dataset tensor: (6204, 32, 32, 3)\n",
      "Mean: -0.10468332\n",
      "Standard deviation: 0.2953001\n",
      "Pickling flickrData\\processedF\\Vodafone\\test.pickle.\n",
      "flickrData\\processedF\\Vodafone\\test\n",
      "Full dataset tensor: (8679, 32, 32, 3)\n",
      "Mean: 0.05106794\n",
      "Standard deviation: 0.33477575\n",
      "Pickling flickrData\\processedF\\Citroen\\test.pickle.\n",
      "flickrData\\processedF\\Citroen\\test\n",
      "Full dataset tensor: (6930, 32, 32, 3)\n",
      "Mean: 0.09007604\n",
      "Standard deviation: 0.36061132\n",
      "Pickling flickrData\\processedF\\Ferrari\\test.pickle.\n",
      "flickrData\\processedF\\Ferrari\\test\n",
      "Full dataset tensor: (3465, 32, 32, 3)\n",
      "Mean: -0.09995858\n",
      "Standard deviation: 0.3405487\n"
     ]
    }
   ],
   "source": [
    "dirs = [\n",
    "        os.path.join(pp_dir, class_name, 'train')      # Look into all the train folder of the class\n",
    "        for class_name in CLASS_NAME\n",
    "    ]\n",
    "test_dirs = [\n",
    "        os.path.join(pp_dir, class_name, 'test')        # Look into all the test folder of the class\n",
    "        for class_name in CLASS_NAME\n",
    "    ]\n",
    "\n",
    "train_datasets = pickling(dirs)\n",
    "test_datasets = pickling(test_dirs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset, valid_labels, train_dataset, train_labels = combine(train_datasets, train_size, val_size)# function called for merging\n",
    "test_dataset, test_labels = combine(test_datasets, test_size)\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)   # function called for randomizing\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)  \n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 1007944515\n"
     ]
    }
   ],
   "source": [
    "makepickle(train_dataset, train_labels, valid_dataset, valid_labels,test_dataset, test_labels)# function called for making a pickle file.\n",
    "statinfo = os.stat(pickle_file)                         # Shows size of the file \n",
    "print('Compressed pickle size:', statinfo.st_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (70000, 32, 32, 3) (70000, 10)\n",
      "Test set (7000, 32, 32, 3) (7000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_data():\n",
    "    with open(\"logo_dataset.pickle\", 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        X = save['train_dataset']       # assign X as train dataset\n",
    "        Y = save['train_labels']        # assign Y as train labels \n",
    "        X_test = save['test_dataset']   # assign X_test as test dataset\n",
    "        Y_test = save['test_labels']    #assign Y_test as test labels\n",
    "        del save\n",
    "    return [X, X_test], [Y, Y_test]\n",
    "\n",
    "def reformat(dataset, labels):   \n",
    "    dataset = dataset.reshape((-1, 32, 32,3)).astype(np.float32)    # Reformatting shape array to give a scalar value for dataset.  \n",
    "    labels = (np.arange(10) == labels[:, None]).astype(np.float32) \n",
    "    return dataset, labels\n",
    "\n",
    "dataset, labels = read_data()\n",
    "X,Y = reformat(dataset[0], labels[0])\n",
    "X_test, Y_test = reformat(dataset[1], labels[1])\n",
    "print('Training set', X.shape, Y.shape)\n",
    "print('Test set', X_test.shape, Y_test.shape)            \n",
    "\n",
    "# Shuffle the data\n",
    "X, Y = shuffle(X, Y)    # Imported from TFLearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model: Convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data is normalized\n",
    "img_prep = ImagePreprocessing()    \n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Create extra synthetic training data by flipping images on our data set.\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\a\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From D:\\a\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the convolutional network architecture:\n",
    "\n",
    "# 32x32 image is the input with 3 color channels (red, green and blue) with it's mean and standard deviation.\n",
    "network = input_data(shape=[None, 32, 32, 3], data_preprocessing=img_prep, data_augmentation=img_aug)\n",
    "\n",
    "\n",
    "# Convolution 1 with 64 nodes with activation function as rectified linear unit:\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "\n",
    "# Max pooling 1:\n",
    "network = max_pool_2d(network, 2)\n",
    "\n",
    "# Convolution 2 with 128 nodes with activation function as rectified linear unit:\n",
    "network = conv_2d(network, 128, 3, activation='relu')\n",
    "\n",
    "# Convolution 3 with 256 nodes with activation function as rectified linear unit:\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "\n",
    "# Max pooling 2:\n",
    "network = max_pool_2d(network, 2)\n",
    "\n",
    "# Fully-connected 512 node neural network with activation function as rectified linear unit\n",
    "network = fully_connected(network, 512 , activation='relu')\n",
    "\n",
    "# Dropout - throw away some data randomly during training to prevent over-fitting\n",
    "network = dropout(network, 0.5)\n",
    "\n",
    "# Fully-connected neural network for outputs with activation function as softmax as we are dealing with multiclass classification.\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "\n",
    "# To train the network we will use adaptive moment estimation (ADAM) and categorical_crossentropy  \n",
    "# to determine loss in learning process and optimization.\n",
    "network = regression(network, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)\n",
    "\n",
    "# Covering the network in a model object\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0, checkpoint_path=\"model\\logo-classifier.tfl.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5469  | total loss: \u001b[1m\u001b[32m0.23238\u001b[0m\u001b[0m | time: 2388.690s\n",
      "| Adam | epoch: 010 | loss: 0.23238 - acc: 0.9842 -- iter: 69888/70000\n",
      "Training Step: 5470  | total loss: \u001b[1m\u001b[32m0.20941\u001b[0m\u001b[0m | time: 2466.489s\n",
      "| Adam | epoch: 010 | loss: 0.20941 - acc: 0.9858 | val_loss: 0.01089 - val_acc: 0.9964 -- iter: 70000/70000\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\aMaL\\Desktop\\logo\\model\\logo-classifier.tfl.ckpt-5470 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:C:\\Users\\aMaL\\Desktop\\logo\\logo-classifier.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Network trained and saved as logo-classifier.tfl!\n"
     ]
    }
   ],
   "source": [
    "# Training! n_epoch will tell how many iterations the network has to go through, here it is kept 15 training passes and monitor it as it goes.\n",
    "model.fit(X,Y, n_epoch=10, shuffle=True, validation_set=(X_test, Y_test), show_metric=True, batch_size=128, snapshot_epoch=True,\n",
    "          run_id='logo-classifier')\n",
    "\n",
    "# Save model when training is complete to a file\n",
    "model.save(\"logo-classifier.tfl\")\n",
    "print(\"Network trained and saved as logo-classifier.tfl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\aMaL\\Desktop\\logo\\model\\logo-classifier.tfl.ckpt-4923\n"
     ]
    }
   ],
   "source": [
    "#Loading the trained dataset file 'logo-classifier.tfl.ckpt-4923'\n",
    "model.load(\"model\\logo-classifier.tfl.ckpt-4923\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9964285714285714]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score=model.evaluate(X_test, Y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
